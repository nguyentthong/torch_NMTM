{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "246a13b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import scipy.sparse\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from tqdm import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "872b09af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextData(Dataset):\n",
    "    def __init__(self, data_dir, partition='train'):\n",
    "        self.partition = partition\n",
    "        self.texts_en, self.bow_matrix_en, self.vocab_en, self.word2id_en, self.id2word_en = self.read_data(data_dir, lang='en')\n",
    "        self.texts_cn, self.bow_matrix_cn, self.vocab_cn, self.word2id_cn, self.id2word_cn = self.read_data(data_dir, lang='cn')\n",
    "        \n",
    "        self.size_en = len(self.texts_en)\n",
    "        self.size_cn = len(self.texts_cn)\n",
    "        self.vocab_size_en = len(self.vocab_en)\n",
    "        self.vocab_size_cn = len(self.vocab_cn)\n",
    "        \n",
    "        self.trans_dict, self.trans_matrix_en, self.trans_matrix_cn = self.parse_dictionary()\n",
    "        \n",
    "        self.Map_en2cn = self.get_Map(self.trans_matrix_en, self.bow_matrix_en)\n",
    "        self.Map_cn2en = self.get_Map(self.trans_matrix_cn, self.bow_matrix_cn)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_en = self.bow_matrix_en[idx]\n",
    "        batch_cn = self.bow_matrix_cn[idx]\n",
    "        return torch.tensor(batch_en, dtype=torch.float32), torch.tensor(batch_cn, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size_en\n",
    "        \n",
    "    def read_text(self, path):\n",
    "        texts = []\n",
    "        with open(path, 'r') as f:\n",
    "            for line in f: texts.append(line.strip())\n",
    "        return texts\n",
    "\n",
    "    def read_data(self, data_dir, lang):\n",
    "        texts = self.read_text(os.path.join(data_dir, '{}_texts_{}.txt'.format(self.partition,lang)))\n",
    "        vocab = self.read_text(os.path.join(data_dir, 'vocab_{}'.format(lang)))\n",
    "        word2id = dict(zip(vocab, range(len(vocab))))\n",
    "        id2word = dict(zip(range(len(vocab)), vocab))\n",
    "\n",
    "        bow_matrix = scipy.sparse.load_npz(os.path.join(data_dir, '{}_bow_matrix_{}.npz'.format(self.partition,lang))).toarray()\n",
    "        return texts, bow_matrix, vocab, word2id, id2word\n",
    "    \n",
    "    def parse_dictionary(self):\n",
    "        trans_dict = defaultdict(set)\n",
    "        trans_matrix_en = np.zeros((self.vocab_size_en, self.vocab_size_cn), dtype='int32')\n",
    "        trans_matrix_cn = np.zeros((self.vocab_size_cn, self.vocab_size_en), dtype='int32')\n",
    "        \n",
    "        with open('./ch_en_dict.dat') as f:\n",
    "            for line in f:\n",
    "                terms = (line.strip()).split()\n",
    "                if len(terms) == 2:\n",
    "                    cn_term = terms[0]\n",
    "                    en_term = terms[1]\n",
    "                    if cn_term in self.word2id_cn and en_term in self.word2id_en:\n",
    "                        trans_dict[cn_term].add(en_term)\n",
    "                        trans_dict[en_term].add(cn_term)\n",
    "                        cn_term_id = self.word2id_cn[cn_term]\n",
    "                        en_term_id = self.word2id_en[en_term]\n",
    "\n",
    "                        trans_matrix_en[en_term_id][cn_term_id] = 1\n",
    "                        trans_matrix_cn[cn_term_id][en_term_id] = 1\n",
    "\n",
    "        return trans_dict, trans_matrix_en, trans_matrix_cn\n",
    "    \n",
    "    def get_Map(self, trans_matrix, bow_matrix):\n",
    "        Map = (trans_matrix * bow_matrix.sum(0)[:, np.newaxis]).astype('float32')\n",
    "        Map = Map + 1\n",
    "        Map_sum = np.sum(Map, axis=1)\n",
    "        t_index = Map_sum > 0\n",
    "        Map[t_index, :] = Map[t_index, :] / Map_sum[t_index, np.newaxis]\n",
    "        return torch.tensor(Map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6d0b53c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainDocSet = TextData('./data/Amazon_Review','train')\n",
    "testDocSet = TextData('./data/Amazon_Review','test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93cd74ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function torch.nn.functional.batch_norm(input, running_mean, running_var, weight=None, bias=None, training=False, momentum=0.1, eps=1e-05)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.functional.batch_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "9c8b0b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NMTM(nn.Module):\n",
    "    def __init__(self, config, Map_en2cn, Map_cn2en):\n",
    "        super(NMTM, self).__init__()\n",
    "        self.config = config\n",
    "        self.Map_en2cn = Map_en2cn\n",
    "        self.Map_cn2en = Map_cn2en\n",
    "        \n",
    "        # encoder\n",
    "        self.phi_cn = nn.Parameter(torch.randn(self.config['topic_num'], self.config['vocab_size_cn']))\n",
    "        self.phi_en = nn.Parameter(torch.randn(self.config['topic_num'], self.config['vocab_size_en']))\n",
    "        \n",
    "        self.W_cn = nn.Parameter(torch.randn(self.config['vocab_size_cn'], self.config['e1']))\n",
    "        self.W_en = nn.Parameter(torch.randn(self.config['vocab_size_en'], self.config['e1']))\n",
    "        \n",
    "        self.B_cn = nn.Parameter(torch.randn(self.config['e1']))\n",
    "        self.B_en = nn.Parameter(torch.randn(self.config['e1']))\n",
    "        \n",
    "        self.act_fun = nn.Softplus()\n",
    "        self.softmax = nn.Softmax()\n",
    "        self.dropout = nn.Dropout(p=0.0)\n",
    "        self.batch_norm_encode_en = nn.BatchNorm1d(self.config['topic_num'])\n",
    "        self.batch_norm_encode_cn = nn.BatchNorm1d(self.config['topic_num'])\n",
    "                \n",
    "        self.W2 = nn.Parameter(torch.randn(self.config['e1'], self.config['e2']))\n",
    "        self.B2 = nn.Parameter(torch.randn(self.config['e2']))\n",
    "        \n",
    "        self.W_m = nn.Parameter(torch.randn(self.config['e2'], self.config['topic_num']))\n",
    "        self.B_m = nn.Parameter(torch.randn(self.config['topic_num']))\n",
    "        \n",
    "        self.W_s = nn.Parameter(torch.randn(self.config['e2'], self.config['topic_num']))\n",
    "        self.B_s = nn.Parameter(torch.randn(self.config['topic_num']))\n",
    "        \n",
    "        self.init_params()\n",
    "        # decoder\n",
    "        beta_cn = (self.config['lam'] * torch.matmul(self.phi_en, self.Map_en2cn) + (1-self.config['lam']) * self.phi_cn).detach()\n",
    "        self.beta_cn = nn.Parameter(beta_cn)\n",
    "        beta_en = (self.config['lam'] * torch.matmul(self.phi_cn, self.Map_cn2en) + (1-self.config['lam']) * self.phi_en).detach()\n",
    "        self.beta_en = nn.Parameter(beta_en)\n",
    "        \n",
    "\n",
    "        self.batch_norm_decode_en = nn.BatchNorm1d(self.config['vocab_size_en'])\n",
    "        self.batch_norm_decode_cn = nn.BatchNorm1d(self.config['vocab_size_cn'])\n",
    "        \n",
    "        # loss\n",
    "        self.a = 1 * torch.ones((1, int(self.config['topic_num'])))\n",
    "        self.mu_priori = nn.Parameter((torch.log(self.a).T - torch.mean(torch.log(self.a),1).T).T, requires_grad=False)\n",
    "        sigma_priori = (((1.0/self.a)*(1-(2.0/self.config['topic_num']))).T + \n",
    "                            (1.0/(self.config['topic_num']*self.config['topic_num']))*torch.sum(1.0/self.a, 1)).T\n",
    "        self.sigma_priori = nn.Parameter(sigma_priori, requires_grad=False)\n",
    "        \n",
    "    def init_params(self):\n",
    "        nn.init.xavier_uniform_(self.phi_cn)\n",
    "        nn.init.xavier_uniform_(self.phi_en)\n",
    "        \n",
    "        nn.init.xavier_uniform_(self.W_cn)\n",
    "        nn.init.xavier_uniform_(self.W_en)\n",
    "        nn.init.zeros_(self.B_cn)\n",
    "        nn.init.zeros_(self.B_en)\n",
    "        \n",
    "        nn.init.xavier_uniform_(self.W2)\n",
    "        nn.init.xavier_uniform_(self.W_m)     \n",
    "        nn.init.xavier_uniform_(self.W_s)\n",
    "        \n",
    "        nn.init.zeros_(self.B2)\n",
    "        nn.init.zeros_(self.B_m)     \n",
    "        nn.init.zeros_(self.B_s)   \n",
    "        \n",
    "\n",
    "    def encode(self, x, lang):\n",
    "        if lang == 'en': \n",
    "            h = self.act_fun(torch.matmul(x, self.W_en) + self.B_en)\n",
    "            batch_norm = self.batch_norm_encode_en\n",
    "        else: \n",
    "            h = self.act_fun(torch.matmul(x, self.W_cn) + self.B_cn)\n",
    "            batch_norm = self.batch_norm_encode_cn\n",
    "        \n",
    "        h = self.act_fun(torch.matmul(h, self.W2) + self.B2)\n",
    "        h = self.dropout(h)\n",
    "        \n",
    "        mean = batch_norm(torch.matmul(h, self.W_m) + self.B_m)\n",
    "        log_sigma_sq = batch_norm(torch.matmul(h, self.W_s) + self.B_s)\n",
    "        val = torch.sqrt(torch.exp(log_sigma_sq))\n",
    "        eps = torch.zeros_like(val).normal_()\n",
    "        z = mean + torch.mul(val, eps)\n",
    "        z = self.softmax(z)\n",
    "        z = self.dropout(z)\n",
    "        \n",
    "        return z, mean, log_sigma_sq\n",
    "    \n",
    "    def decode(self, z, beta, lang):\n",
    "        if lang == 'en': \n",
    "            batch_norm = self.batch_norm_decode_en\n",
    "        else: \n",
    "            batch_norm = self.batch_norm_decode_cn\n",
    "        \n",
    "        x_recon = self.softmax(batch_norm(torch.matmul(z, beta)))\n",
    "        return x_recon\n",
    "    \n",
    "    def get_loss(self, x, x_recon, z_mean, z_log_sigma_sq):\n",
    "        sigma = torch.exp(z_log_sigma_sq)\n",
    "        latent_loss = 0.5 * (torch.sum(torch.div(sigma, self.sigma_priori),1) + \\\n",
    "                        torch.sum(torch.mul(torch.div((self.mu_priori - z_mean), self.sigma_priori), (self.mu_priori - z_mean)), 1) \n",
    "                             - self.config['topic_num'] + torch.sum(torch.log(self.sigma_priori), 1) \n",
    "                             - torch.sum(z_log_sigma_sq, 1))\n",
    "        recon_loss = torch.sum(-x * torch.log(x_recon), axis=1)\n",
    "        loss = latent_loss + recon_loss\n",
    "        return loss.mean()\n",
    "    \n",
    "    def forward(self, x_cn, x_en):\n",
    "        # encode\n",
    "        z_cn, z_mean_cn, z_log_sigma_sq_cn = self.encode(x_cn, 'cn')\n",
    "        z_en, z_mean_en, z_log_sigma_sq_en = self.encode(x_en, 'en')\n",
    "        \n",
    "        # decode\n",
    "        x_recon_cn = self.decode(z_cn, self.beta_cn, 'cn')\n",
    "        x_recon_en = self.decode(z_en, self.beta_en, 'en')\n",
    "        \n",
    "        return z_cn, z_mean_cn, z_log_sigma_sq_cn, z_en, z_mean_en, z_log_sigma_sq_en, x_recon_cn, x_recon_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "55dfa0a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "config = dict()\n",
    "config['topic_num'] = 20\n",
    "config['batch_size'] = 128\n",
    "config['epoch'] = 1\n",
    "config['e1'] = 100\n",
    "config['e2'] = 100\n",
    "config['vocab_size_en'] = trainDocSet[0][0].size(0)\n",
    "config['vocab_size_cn'] = trainDocSet[0][1].size(0)\n",
    "config['lam'] = 0.8\n",
    "config['learning_rate'] = 0.001\n",
    "config['output_dir'] = './output'\n",
    "\n",
    "model = NMTM(config, trainDocSet.Map_en2cn, trainDocSet.Map_cn2en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "59852c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(config, model, dataset):\n",
    "    lr = config['learning_rate']\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    train_loader = DataLoader(dataset, batch_size=config['batch_size'], shuffle=True)\n",
    "    for epoch in trange(config['epoch']):\n",
    "        for idx, batch in tqdm(enumerate(train_loader)):\n",
    "            optimizer.zero_grad()\n",
    "            batch_data_en, batch_data_cn = batch\n",
    "            z_cn, z_mean_cn, z_log_sigma_sq_cn, z_en, z_mean_en, z_log_sigma_sq_en, x_recon_cn, x_recon_en = model(batch_data_en, batch_data_cn)\n",
    "\n",
    "            # get_loss\n",
    "            loss_cn = model.get_loss(batch_data_cn, x_recon_cn, z_mean_cn, z_log_sigma_sq_cn)\n",
    "            loss_en = model.get_loss(batch_data_en, x_recon_en, z_mean_en, z_log_sigma_sq_en)\n",
    "            loss = loss_cn + loss_en\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print('Epoch {} \\t Loss: {}'.format(epoch, loss))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "979dd489",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                               | 0/1 [00:00<?, ?it/s]\n",
      "0it [00:00, ?it/s]\u001b[A/home/ubuntu/anaconda3/envs/myenv/lib/python3.7/site-packages/ipykernel_launcher.py:85: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home/ubuntu/anaconda3/envs/myenv/lib/python3.7/site-packages/ipykernel_launcher.py:96: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "\n",
      "1it [00:00,  1.59it/s]\u001b[A\n",
      "2it [00:01,  1.32it/s]\u001b[A\n",
      "3it [00:01,  1.54it/s]\u001b[A\n",
      "4it [00:02,  1.38it/s]\u001b[A\n",
      "5it [00:03,  1.39it/s]\u001b[A\n",
      "6it [00:04,  1.28it/s]\u001b[A\n",
      "7it [00:05,  1.35it/s]\u001b[A\n",
      "8it [00:05,  1.30it/s]\u001b[A\n",
      "9it [00:06,  1.33it/s]\u001b[A\n",
      "10it [00:07,  1.35it/s]\u001b[A\n",
      "11it [00:08,  1.30it/s]\u001b[A\n",
      "12it [00:08,  1.33it/s]\u001b[A\n",
      "13it [00:09,  1.33it/s]\u001b[A\n",
      "14it [00:10,  1.32it/s]\u001b[A\n",
      "15it [00:11,  1.35it/s]\u001b[A\n",
      "16it [00:12,  1.28it/s]\u001b[A\n",
      "17it [00:12,  1.31it/s]\u001b[A\n",
      "18it [00:13,  1.32it/s]\u001b[A\n",
      "19it [00:14,  1.34it/s]\u001b[A\n",
      "20it [00:14,  1.45it/s]\u001b[A\n",
      "21it [00:15,  1.32it/s]\u001b[A\n",
      "22it [00:16,  1.42it/s]\u001b[A\n",
      "23it [00:17,  1.33it/s]\u001b[A\n",
      "24it [00:17,  1.34it/s]\u001b[A\n",
      "25it [00:18,  1.28it/s]\u001b[A\n",
      "26it [00:19,  1.34it/s]\u001b[A\n",
      "27it [00:20,  1.26it/s]\u001b[A\n",
      "28it [00:20,  1.31it/s]\u001b[A\n",
      "29it [00:21,  1.36it/s]\u001b[A\n",
      "30it [00:22,  1.34it/s]\u001b[A\n",
      "31it [00:23,  1.28it/s]\u001b[A\n",
      "32it [00:24,  1.28it/s]\u001b[A\n",
      "33it [00:24,  1.26it/s]\u001b[A\n",
      "34it [00:25,  1.28it/s]\u001b[A\n",
      "35it [00:26,  1.29it/s]\u001b[A\n",
      "36it [00:27,  1.26it/s]\u001b[A\n",
      "37it [00:27,  1.29it/s]\u001b[A\n",
      "38it [00:28,  1.28it/s]\u001b[A\n",
      "39it [00:29,  1.29it/s]\u001b[A\n",
      "40it [00:30,  1.41it/s]\u001b[A\n",
      "41it [00:30,  1.36it/s]\u001b[A\n",
      "42it [00:31,  1.39it/s]\u001b[A\n",
      "43it [00:32,  1.54it/s]\u001b[A\n",
      "44it [00:32,  1.39it/s]\u001b[A\n",
      "45it [00:33,  1.52it/s]\u001b[A\n",
      "46it [00:34,  1.32it/s]\u001b[A\n",
      "47it [00:35,  1.42it/s]\u001b[A\n",
      "48it [00:35,  1.38it/s]\u001b[A\n",
      "49it [00:36,  1.43it/s]\u001b[A\n",
      "50it [00:37,  1.33it/s]\u001b[A\n",
      "51it [00:38,  1.33it/s]\u001b[A\n",
      "52it [00:38,  1.25it/s]\u001b[A\n",
      "53it [00:39,  1.31it/s]\u001b[A\n",
      "54it [00:40,  1.27it/s]\u001b[A\n",
      "55it [00:41,  1.27it/s]\u001b[A\n",
      "56it [00:42,  1.28it/s]\u001b[A\n",
      "57it [00:42,  1.23it/s]\u001b[A\n",
      "58it [00:43,  1.23it/s]\u001b[A\n",
      "59it [00:44,  1.21it/s]\u001b[A\n",
      "60it [00:45,  1.29it/s]\u001b[A\n",
      "61it [00:45,  1.36it/s]\u001b[A\n",
      "62it [00:46,  1.41it/s]\u001b[A\n",
      "63it [00:47,  1.50it/s]\u001b[A\n",
      "64it [00:47,  1.50it/s]\u001b[A\n",
      "65it [00:48,  1.43it/s]\u001b[A\n",
      "66it [00:49,  1.42it/s]\u001b[A\n",
      "67it [00:50,  1.38it/s]\u001b[A\n",
      "68it [00:50,  1.33it/s]\u001b[A\n",
      "69it [00:51,  1.27it/s]\u001b[A\n",
      "70it [00:52,  1.24it/s]\u001b[A\n",
      "71it [00:53,  1.24it/s]\u001b[A\n",
      "72it [00:54,  1.20it/s]\u001b[A\n",
      "73it [00:55,  1.23it/s]\u001b[A\n",
      "74it [00:55,  1.32it/s]\u001b[A\n",
      "75it [00:56,  1.26it/s]\u001b[A\n",
      "76it [00:57,  1.39it/s]\u001b[A\n",
      "77it [00:57,  1.32it/s]\u001b[A\n",
      "78it [00:58,  1.45it/s]\u001b[A\n",
      "79it [00:59,  1.40it/s]\u001b[A\n",
      "80it [01:00,  1.31it/s]\u001b[A\n",
      "81it [01:00,  1.27it/s]\u001b[A\n",
      "82it [01:01,  1.31it/s]\u001b[A\n",
      "83it [01:02,  1.30it/s]\u001b[A\n",
      "84it [01:03,  1.23it/s]\u001b[A\n",
      "85it [01:04,  1.23it/s]\u001b[A\n",
      "86it [01:04,  1.23it/s]\u001b[A\n",
      "87it [01:05,  1.32it/s]\u001b[A\n",
      "88it [01:06,  1.27it/s]\u001b[A\n",
      "89it [01:07,  1.35it/s]\u001b[A\n",
      "90it [01:07,  1.51it/s]\u001b[A\n",
      "91it [01:08,  1.40it/s]\u001b[A\n",
      "92it [01:09,  1.47it/s]\u001b[A\n",
      "93it [01:09,  1.39it/s]\u001b[A\n",
      "94it [01:10,  1.35it/s]\u001b[A\n",
      "95it [01:11,  1.36it/s]\u001b[A\n",
      "96it [01:12,  1.31it/s]\u001b[A\n",
      "97it [01:12,  1.33it/s]\u001b[A\n",
      "98it [01:13,  1.35it/s]\u001b[A\n",
      "99it [01:14,  1.39it/s]\u001b[A\n",
      "100it [01:14,  1.44it/s]\u001b[A\n",
      "101it [01:15,  1.41it/s]\u001b[A\n",
      "102it [01:16,  1.51it/s]\u001b[A\n",
      "103it [01:17,  1.32it/s]\u001b[A\n",
      "104it [01:17,  1.43it/s]\u001b[A\n",
      "105it [01:18,  1.36it/s]\u001b[A\n",
      "106it [01:19,  1.36it/s]\u001b[A\n",
      "107it [01:20,  1.35it/s]\u001b[A\n",
      "108it [01:20,  1.38it/s]\u001b[A\n",
      "109it [01:21,  1.35it/s]\u001b[A\n",
      "110it [01:22,  1.33it/s]\u001b[A\n",
      "111it [01:23,  1.35it/s]\u001b[A\n",
      "112it [01:23,  1.47it/s]\u001b[A\n",
      "113it [01:24,  1.35it/s]\u001b[A\n",
      "114it [01:25,  1.45it/s]\u001b[A\n",
      "115it [01:25,  1.37it/s]\u001b[A\n",
      "116it [01:26,  1.44it/s]\u001b[A\n",
      "117it [01:27,  1.50it/s]\u001b[A\n",
      "118it [01:27,  1.36it/s]\u001b[A\n",
      "119it [01:28,  1.48it/s]\u001b[A\n",
      "120it [01:29,  1.40it/s]\u001b[A\n",
      "121it [01:29,  1.48it/s]\u001b[A\n",
      "122it [01:30,  1.57it/s]\u001b[A\n",
      "123it [01:31,  1.37it/s]\u001b[A\n",
      "124it [01:31,  1.48it/s]\u001b[A\n",
      "125it [01:32,  1.39it/s]\u001b[A\n",
      "126it [01:33,  1.35it/s]\u001b[A\n",
      "127it [01:34,  1.38it/s]\u001b[A\n",
      "128it [01:35,  1.29it/s]\u001b[A\n",
      "129it [01:35,  1.41it/s]\u001b[A\n",
      "130it [01:36,  1.38it/s]\u001b[A\n",
      "131it [01:37,  1.37it/s]\u001b[A\n",
      "132it [01:37,  1.40it/s]\u001b[A\n",
      "133it [01:38,  1.39it/s]\u001b[A\n",
      "134it [01:39,  1.29it/s]\u001b[A\n",
      "135it [01:40,  1.35it/s]\u001b[A\n",
      "136it [01:41,  1.26it/s]\u001b[A\n",
      "137it [01:41,  1.30it/s]\u001b[A\n",
      "138it [01:42,  1.27it/s]\u001b[A\n",
      "139it [01:43,  1.36it/s]\u001b[A\n",
      "140it [01:43,  1.41it/s]\u001b[A\n",
      "141it [01:44,  1.43it/s]\u001b[A\n",
      "142it [01:45,  1.33it/s]\u001b[A\n",
      "143it [01:46,  1.39it/s]\u001b[A\n",
      "144it [01:46,  1.37it/s]\u001b[A\n",
      "145it [01:47,  1.36it/s]\u001b[A\n",
      "146it [01:48,  1.48it/s]\u001b[A\n",
      "147it [01:49,  1.35it/s]\u001b[A\n",
      "148it [01:49,  1.34it/s]\u001b[A\n",
      "149it [01:50,  1.32it/s]\u001b[A\n",
      "150it [01:51,  1.26it/s]\u001b[A\n",
      "151it [01:52,  1.22it/s]\u001b[A\n",
      "152it [01:53,  1.19it/s]\u001b[A\n",
      "153it [01:53,  1.27it/s]\u001b[A\n",
      "154it [01:54,  1.33it/s]\u001b[A\n",
      "155it [01:55,  1.32it/s]\u001b[A\n",
      "156it [01:56,  1.30it/s]\u001b[A\n",
      "157it [01:56,  1.35it/s]\u001b[A\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [01:56<00:00, 116.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 \t Loss: 638.4900512695312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = train(config, model, trainDocSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "53d1b48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_words(config, beta, id2word, lang, n_top_words=15):\n",
    "    top_words = []\n",
    "    for i in range(len(beta)):\n",
    "        top_words.append(\" \".join([id2word[j] for j in beta[i].argsort().tolist()[:-n_top_words-1:-1]]))\n",
    "    \n",
    "    with open(os.path.join(config['output_dir'], 'top_words_T{}_K{}_{}'.format(n_top_words, config['topic_num'], lang)), 'w') as f:\n",
    "        for line in top_words:\n",
    "            f.write(line + '\\n')\n",
    "            print(line)\n",
    "\n",
    "def export_beta(config, model, data):\n",
    "    beta_en, beta_cn = model.beta_en, model.beta_cn\n",
    "    print_top_words(config, beta_en, data.id2word_en, lang='en')\n",
    "    print_top_words(config, beta_cn, data.id2word_cn, lang='cn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee30540",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "like works books want ever bought one detail really fit even made two written printer\n",
      "recommend expect worth good feel price old fit found excellent acid characters change conclusion important\n",
      "day go parts back review floor holy house value instead like quality behind amazon times\n",
      "within found todays provide similar another software last guide looked shelf specifically captain edge singer\n",
      "year know book really order got low read new people truly im friend detail even\n",
      "movie well said book feel film made think started buy funny keep first much went\n",
      "cant youre old want plot quality waste reviews music book operating prize actual someone world\n",
      "sound bad old every without making set money comparing ever used go nice type work\n",
      "think story though flaw set little read back less characters band enough however law thrill\n",
      "maybe could worked see product songs sound great player short difficult directions enjoyed next together\n",
      "cd get must song youll great box way may release right best john advice stuff\n",
      "would top time days way dvd everything easy good left stickers instead defective bit sounds\n",
      "however give last noise feels many years days etc falls visit business daddy im things\n",
      "make went want much also little etc many see things take named cheapest kind expensive\n",
      "junk isnt every time easy steal need paper support receipt six problem done close item\n",
      "review little never time copy original editions beta author mixer right use cgi realize extremely\n",
      "buy dont life think old use problem bit buying also disappointed spent low books mr\n",
      "every didnt right thing self dont screen last d going anything little first fact enough\n",
      "picture didnt coffee feel wouldnt away fine need rate work fast going hooked countries used\n",
      "one good game know people information danger thought bad music two look wrong first guidance\n",
      "此书 功能 条 想 一对 不错 原因 最 贵 订 严肃 完成 笑话 所有 书友\n",
      "孩子 歌 英文 张杰 原来 收到 首 免 小 全部 感觉 情感 送给 花 支持\n",
      "作品 读 现在 推荐 真 听 值得 大学 周年 先生 知道 却 中医 想 唱\n",
      "一些 看到 风险 内容 进行 内心 公司 大于 发展 现在 每个 不错 女人 主角 古典\n",
      "发现 小说 检测 心理 版本 一些 分 中 满意 吃 实际 建筑 帮助 心 钱\n",
      "款 好像 知道 词 价格 已经 卓越 产品 一下 成 岁 功能 做 印象 宇宙\n",
      "故事 比较 力 做法 完 时间 适合 毛 收到 描写 名字 文件 东京 手 儿子\n",
      "好好 质量 对方 巫师 过去 形 凤凰 钟 学 却 放 普罗旺斯 效果 看到 美丽\n",
      "点 歌 看看 不好 经典 作者 段 平均 明白 第一 危机 表现 珍贵 读 爱\n",
      "水 时 专辑 值 小 款 分析 看待 文化 专业 希望 例子 农村 事情 购买\n",
      "后 中国 东西 一直 生活 历史 小 低级 卓越 社会 问题 已 听 中 存在\n",
      "老子 堡垒 坐 元 政治 康 太空 已 想 令 油 哲学 好闻 好人 一对\n",
      "学校 认为 周 大小 词典 学习 约 看到 培养 戴 票房 票 英汉 演出 双解\n",
      "奶奶 质量 差 表 贵 帮助 灌水 适合 袋 人们 互联网 一番 远 报价 款\n",
      "书 一对 服务 满意 知道 谢谢 现在 张 实在 接触 十 意见 首 卓越 一点\n",
      "觉得 世界 强 谢谢 一定 实践 享受 显示 包装 主动 真的 权威 君 生活 特点\n",
      "问题 中国 高 日 精彩 灌水 一下 世界 真的 谢谢 情况 网 当时 战争 影响\n",
      "作者 第一 发现 里面 一对 校园 换 系统 超 值得 中国 卓越 请 纸张 讲解\n",
      "希望 成 不错 吸引 配送 莫名 管理 答应 瓶 专辑 貌似 真正 翔实 同感 反映\n",
      "写 气 点 张 章 觉得 强 过程 爱情 浪漫 短 纪 外表 重 后\n"
     ]
    }
   ],
   "source": [
    "export_beta(config, model, trainDocSet)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
